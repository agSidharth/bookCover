{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-28T17:13:45.153469Z","iopub.execute_input":"2022-11-28T17:13:45.153878Z","iopub.status.idle":"2022-11-28T17:13:58.920274Z","shell.execute_reply.started":"2022-11-28T17:13:45.153797Z","shell.execute_reply":"2022-11-28T17:13:58.919151Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\n# Importing libraries\nimport numpy as np \nimport pandas as pd\nimport os\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport sys\n\ninputPath = '/kaggle/input/col774-2022'\n\n# Initializing seed to maintain consistency\nSEED = 661077\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\n# Using various hyperparameters \nEPOCHS = 2\nEPSILON = 1e-4\nDEBUG = True \nLR = 0.01\nMOMENTUM = 0.9\n\n# Take cude if it is available\nthisDevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device(thisDevice)\nprint(\"The device in use is : \"+thisDevice)\n\n# Function for dataset class\nclass bookDataset(Dataset):\n    def __init__(self,images_file,labels_file,dataDir,comp = False):\n        # store the dataframe for the csv files\n        \n        self.images = pd.read_csv(os.path.join(dataDir,images_file))\n        \n        # comp part labels are not given\n        self.comp = comp\n        self.labels = None\n        \n        if not self.comp:\n            self.labels = pd.read_csv(os.path.join(dataDir,labels_file))\n        \n        self.img_dir = os.path.join(dataDir,\"images\",\"images\")\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self,idx):\n        img_path = os.path.join(self.img_dir,self.images.iloc[idx,1])\n        thisImg = Image.open(img_path)\n        thisTransform = transforms.Compose([transforms.PILToTensor()])\n        img_tensor = thisTransform(thisImg)/255\n        \n        thisLab = -1\n        if not self.comp:\n            thisLab = self.labels.iloc[idx,1]\n        return img_tensor,thisLab\n\n# dataset & dataloaders initialization\ntrainDataset = bookDataset(\"train_x.csv\",\"train_y.csv\",inputPath)\ntestDataset = bookDataset(\"non_comp_test_x.csv\",\"non_comp_test_y.csv\",inputPath)\ntrainloader = DataLoader(trainDataset,batch_size = 64,shuffle = True)\ntestloader = DataLoader(testDataset,batch_size = 64,shuffle = True)\n\n\n# visualization of data points\nprint(trainDataset[0][0].size())\n#plt.imshow(trainDataset[0][0].numpy().transpose(1, 2, 0))\n\n# model class\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel,self).__init__()\n        \n        self.conv = nn.Sequential(\n            nn.Conv2d(3,32,kernel_size = (5,5)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = (2,2),stride = 2),\n            nn.Conv2d(32,64,kernel_size = (5,5)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = (2,2),stride = 2),\n            nn.Conv2d(64,128,kernel_size = (5,5)),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = (2,2),stride = 2))\n        \n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128*24*24,128),\n            nn.ReLU(),\n            nn.Linear(128,30))\n        \n    \n    def forward(self,x):\n        convOut = self.conv(x)\n        fcOut = self.fc(convOut)\n        return fcOut\n\nmodel = CNNModel()\n\n# defining the loss function and optimizer for the model\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(),lr = LR,momentum = MOMENTUM)   \n\nmodel = model.to(device)\n\n# Testing on the test data\ndef calAccuracy(model,dataloader,device):\n    correct = 0\n    total = 0\n    \n    # no need to track the forward computation\n    torch.set_grad_enabled(False)\n    \n    for data in dataloader:\n        inputs,labels = data\n        total += labels.size(0)\n        \n        inputs,labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        _,predicted = torch.max(model(inputs).data,1)\n        correct += (predicted==labels).sum().item()\n        \n    torch.set_grad_enabled(True)\n    finalAcc = (correct*100)/total\n    print(\"The accuracy of the model : \"+str(finalAcc))\n    return finalAcc\n\n\n# training model function\ndef trainModel(model,dataloader,loss_fn,optimizer,EPOCHS,EPSILON,device,PATH):\n    \n    last_loss = (np.inf)/2\n    max_valAcc = 0\n    \n    for epoch in range(EPOCHS):\n        this_loss = 0.0\n\n        for idx,data in enumerate(dataloader,0):\n            if(DEBUG and idx%100==0): print(\"Iteration : \"+str(idx))\n            inputs,labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n            loss = loss_fn(model(inputs),labels)\n            loss.backward()\n            optimizer.step()\n\n            this_loss += loss\n        \n        this_loss = this_loss/len(dataloader)\n        #if(abs(this_loss-last_loss)<EPSILON): break\n        last_loss = this_loss\n        \n        print(\"Epoch : \"+str(epoch)+\", Loss ==> \"+str(last_loss))\n        print(\"Testing Accuracy ==>\")\n        \n        this_valAcc = calAccuracy(model,testloader,device)\n        \n        # Save the model only if validation accuracy is greater than previous max\n        if(this_valAcc>max_valAcc):\n            max_valAcc = this_valAcc\n            torch.save(model.state_dict(),PATH)\n            print(\"Model saved\")\n        \n    return model\n\n# Running the loop\nPATH = \"/kaggle/working/CNNModel.pth\"\ntrainModel(model,trainloader,loss_fn,optimizer,EPOCHS,EPSILON,device,PATH)\n\n\n# reloading the last saved(best) model\nmodel = CNNModel()\nmodel.load_state_dict(torch.load(PATH))\nmodel.to(device)\n\n#print(\"Training Accuracy ==>\"+str(calAccuracy(model,trainloader,device)))\n#print(\"Testing Accuracy ==>\"+str(calAccuracy(model,testloader,device)))\n\ncompDataset = bookDataset(\"non_comp_test_x.csv\",\"non_comp_test_y.csv\",inputPath,comp = True)\ncomploader = DataLoader(compDataset,batch_size = 64,shuffle = False)\n\n# printing the output dataframe to csv file\ndef outputToFile(model,dataloader,device,outFileName):\n    torch.set_grad_enabled(False)\n    \n    counter = 0\n    Ids = []\n    outputs = []\n    \n    for data in dataloader:\n        inputs,labels = data\n        inputs = inputs.to(device)\n        _,predicted = torch.max(model(inputs).data,1)\n        \n        for pred in predicted:\n            Ids.append(counter)\n            outputs.append(int(pred))\n            counter += 1\n        \n    torch.set_grad_enabled(True)\n    df = pd.DataFrame(list(zip(Ids,outputs)),columns = [\"Id\",\"Genre\"])\n    df.to_csv(outFileName,index=False)\n\noutputToFile(model,comploader,device,\"/kaggle/working/non_comp_test_pred_y.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch, torchtext\nimport torch.nn as nn\nimport os, sys\nfrom torchtext.data import get_tokenizer\nfrom collections import Counter, OrderedDict\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\nfrom torchvision.io import read_image\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport nltk\nfrom nltk.corpus import stopwords\n\ndataset_dir_path = \"/kaggle/input/col774-2022/\"\n\nclass bookDataset(Dataset):\n    def __init__(self,path_to_x,path_to_y,dataDir):\n        \n        # fetch titles from file\n        self.titles = pd.read_csv(os.path.join(dataDir,path_to_x)).iloc[:,2].values\n        tokenizer = get_tokenizer(\"basic_english\")\n        \n        # tokenize the words\n        self.titles = [tokenizer(data) for data in self.titles]\n        self.labels = []\n        \n        # fetch labels and one hot encode them\n        self.labels = pd.read_csv(os.path.join(dataDir,path_to_y)).iloc[:,1].values\n        self.labels = nn.functional.one_hot(torch.tensor(self.labels),num_classes=30).reshape(-1,30)\n        \n        self.x = self.titles\n        self.y = self.labels\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self,idx):\n        return self.x[idx], self.y[idx]\n    \n    \n# padding to make equal length\ndef collate_fn(batch):\n    inputs = []\n    labels = []\n    \n    for b in batch:\n        inputs.append(torch.tensor(b[0]))\n        labels.append(b[1])\n\n    return torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True), torch.stack(labels)\n\ndef buildVocabulary(x):    \n    # Build the vocabulary\n    all_words = []\n    for tokens in x:\n        for token in tokens:\n            if token.isalpha() and (token not in stopwords.words('english')):\n                all_words.append(token)\n                    \n    counter = Counter(all_words)\n    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n    vocab = torchtext.vocab.vocab(ordered_dict, specials=['<unk>', '<pad>'])\n    vocab.set_default_index(vocab['<unk>'])\n    \n    return vocab\n\n# get indices for all words from the vocabulary\ndef indexWords(x, vocab):\n    for i, tokens in enumerate(x):\n        x[i] = []\n        for token in tokens:\n            if not vocab[token] == 0:\n                x[i].append(vocab[token])\n    return x\n    \n# dataset & dataloaders initialization\ntrainDataset = bookDataset(\"train_x.csv\",\"train_y.csv\",dataset_dir_path)\ntestDataset = bookDataset(\"non_comp_test_x.csv\",\"non_comp_test_y.csv\",dataset_dir_path)\n\nvocab = buildVocabulary(trainDataset.x)\ntrainDataset.x = indexWords(trainDataset.x, vocab)\ntestDataset.x = indexWords(testDataset.x, vocab)\n\ntrainloader = DataLoader(trainDataset,batch_size = 1000,shuffle = True, collate_fn=collate_fn)\ntestloader = DataLoader(testDataset,batch_size = 1000,shuffle = True, collate_fn=collate_fn)\n\n# fetch glove embeddings\nglove = torchtext.vocab.GloVe(name='6B', dim=300)\n\nclass A4_RNN(nn.Module):\n    \n    def __init__(self, input_size=300, hidden_size=128, num_layers=1, num_classes=30):\n        super(A4_RNN, self).__init__()\n        \n        # compute mean glove embedding for missing words\n        mean_embedding = torch.zeros(300)\n        count = 0\n        for word in glove.stoi:\n            mean_embedding += glove[word]\n            count+=1\n\n        mean_embedding = mean_embedding/(count)\n                \n        # make embedding matrix\n        mat = torch.zeros(len(vocab), 300)\n        for i, word in enumerate(vocab.get_stoi()):\n            if word in glove.stoi:\n                mat[i] = glove[word]\n            else:\n                mat[i] = mean_embedding\n        \n        self.embedding = nn.Embedding.from_pretrained(mat, freeze=False)\n        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, nonlinearity='tanh', bias=True, batch_first=True, dropout=0, bidirectional=True)\n        self.FC1 = nn.Linear(hidden_size*2,128)\n        self.FC2 = nn.Linear(128, num_classes)\n    \n    def forward(self, x):\n        x = self.embedding(x.long())\n        x,_ = self.rnn(x)\n        x = self.FC1(x[:,0,:])\n        x = torch.tanh(x)\n        x = self.FC2(x)\n        x = torch.tanh(x)\n        return x\n    \nmodel = A4_RNN()\nmodel.float()\n\n# compute accuracy\ndef calAccuracy(model,dataloader,device):\n    correct = 0\n    total = 0\n    \n    model.eval()\n    \n    for data in dataloader:\n        inputs,labels = data\n        total += labels.size(0)\n        \n        inputs,labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        _,predicted = torch.max(model(inputs).data,1)\n\n        truth = [int(np.argmax(lbl.cpu())) for lbl in labels]\n        \n        for pred,tru in zip(predicted,truth):\n            correct += (pred==tru)\n\n    print(\"The accuracy of the model : \"+str((float(correct)*100)/total))\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nEPOCHS = 20\nEPSILON = 1e-4\nDEBUG = True\n\nthisDevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device(thisDevice)\nmodel = model.to(device)\nprint(\"The device in use is : \"+thisDevice)\n\n# Training the model\ndef trainModel(model,dataloader,loss_fn,optimizer,EPOCHS,EPSILON,device):\n    \n    model.train()\n    \n    last_loss = (np.inf)/2\n    for epoch in range(EPOCHS):\n        this_loss = 0.0\n\n        for idx,data in enumerate(dataloader,0):\n            \n            inputs,labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n                    \n            labels = labels.float()\n            optimizer.zero_grad()\n            loss = loss_fn(model(inputs),labels)\n            \n            loss.backward()\n            optimizer.step()\n\n            this_loss += loss\n        \n        this_loss = this_loss/len(dataloader)\n        last_loss = this_loss\n        \n        if(DEBUG): \n            print(\"Epoch : \"+str(epoch)+\", Loss ==> \"+str(last_loss.item()))\n            print(\"Testing Accuracy ==>\")\n            calAccuracy(model,testloader,device)\n            model.train()\n        \n    return model\n\nmodel = trainModel(model,trainloader,loss_fn,optimizer,EPOCHS,EPSILON,device)\nPATH = \"/kaggle/working/RNNModel.pth\"\ntorch.save(model.state_dict(),PATH)\n\nprint(\"Training Accuracy ==>\")\ncalAccuracy(model, trainloader,device)\nprint(\"Testing Accuracy ==>\")\ncalAccuracy(model,testloader,device)\n\nnoncompDataset = bookDataset(\"non_comp_test_x.csv\",\"non_comp_test_y.csv\",dataset_dir_path)\nnoncompDataset.x = indexWords(noncompDataset.x, vocab)\nnoncomploader = DataLoader(noncompDataset,batch_size = 1000,shuffle = True, collate_fn=collate_fn)\n\n# printing the output dataframe to csv file\ndef outputToFile(model,dataloader,device,outFileName):\n    torch.set_grad_enabled(False)\n    \n    counter = 0\n    Ids = []\n    outputs = []\n    total = 0\n    \n    for idx,data in enumerate(dataloader,0):\n        inputs,labels = data\n        total += labels.size(0)\n        \n        inputs,labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        _,predicted = torch.max(model(inputs).data,1)\n        \n        for pred in predicted:\n            Ids.append(counter)\n            outputs.append(int(pred))\n            counter += 1\n        \n    torch.set_grad_enabled(True)\n    df = pd.DataFrame(list(zip(Ids,outputs)),columns = [\"Id\",\"Genre\"])\n    df.to_csv(outFileName,index=False)\n\noutputToFile(model,noncomploader,device,\"/kaggle/working/non_comp_test_pred_y2.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader\nfrom transformers import EarlyStoppingCallback\n\ninputPath = \"/kaggle/input/col774-2022/\"\n\n# Setting various hyperparameters\nSEED = 661077\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\nMAX_LENGTH = 60\nNUM_LABELS = 30\nmodelName = \"bert-large-uncased\"\n\nEPOCHS = 10\nEPSILON = 1e-4\nDEBUG = True \nLR = 2.5*0.00001\nMOMENTUM = 0.9\n\nthisDevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device(thisDevice)\nprint(\"The device in use is : \"+thisDevice)\n\n\n# inintializing pretrained bert and its tokenizer\ntokenizer = BertTokenizer.from_pretrained(modelName)\nmodel = BertForSequenceClassification.from_pretrained(modelName, num_labels=NUM_LABELS)\n\n\n# Function for data class\nclass bookDataset(Dataset):\n    def __init__(self,path_to_x,path_to_y,dataDir,tokenizer):\n        \n        self.temp_titles = list(pd.read_csv(os.path.join(dataDir,path_to_x)).iloc[:,2].values)\n        self.titles = tokenizer(self.temp_titles,padding = True,truncation = True,max_length = MAX_LENGTH)\n        \n        self.labels = None\n        if path_to_y!=\"\":\n            self.labels = list(pd.read_csv(os.path.join(dataDir,path_to_y)).iloc[:,1].values)\n    \n    def __len__(self):\n        return len(self.titles[\"input_ids\"])\n    \n    def __getitem__(self,idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.titles.items()}\n        if self.labels!=None:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\ntrainDataset = bookDataset(\"train_x.csv\",\"train_y.csv\",inputPath,tokenizer)\ntestDataset = bookDataset(\"non_comp_test_x.csv\",\"non_comp_test_y.csv\",inputPath,tokenizer)\ntrainloader = DataLoader(trainDataset,batch_size = 64,shuffle = True)\ntestloader = DataLoader(testDataset,batch_size = 64,shuffle = True)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n\nmodel = model.to(device)\n\n# Testing on the test data\ndef calAccuracy(model,dataloader,device):\n    correct = 0\n    total = 0\n    \n    torch.set_grad_enabled(False)\n    \n    for data in dataloader:\n        input_ids = data[\"input_ids\"].to(device)\n        attention_mask = data[\"attention_mask\"].to(device)\n        labels = data[\"labels\"].to(device)\n        \n        total += input_ids.size(0)\n        \n        outputs = model(input_ids,attention_mask = attention_mask,labels = labels)\n        predicted = torch.nn.functional.softmax(outputs[\"logits\"]).argmax(1)\n        correct += (predicted==labels).sum().item()\n        \n    torch.set_grad_enabled(True)\n    finalAcc = (correct*100)/total\n    print(\"The accuracy of the model : \"+str(finalAcc))\n    return finalAcc\n\n\n# training model function\ndef trainModel(model,dataloader,loss_fn,optimizer,EPOCHS,EPSILON,device,PATH):\n    \n    last_loss = (np.inf)/2\n    max_valAcc = 0\n    \n    for epoch in range(EPOCHS):\n        this_loss = 0.0\n\n        for idx,data in enumerate(dataloader,0):\n            if(DEBUG and idx%100==0): \n                print(\"Iteration : \"+str(idx))\n#                 calAccuracy(model,testloader,device)\n            \n            input_ids = data[\"input_ids\"].to(device)\n            attention_mask = data[\"attention_mask\"].to(device)\n            labels = data[\"labels\"].to(device)\n            \n            optimizer.zero_grad()\n            output = model(input_ids,attention_mask = attention_mask,labels=labels)\n            loss = output[0]\n            loss.backward()\n            optimizer.step()\n\n            this_loss += loss\n        \n        this_loss = this_loss/len(dataloader)\n        #if(abs(this_loss-last_loss)<EPSILON): break\n        last_loss = this_loss\n        \n        print(\"Epoch : \"+str(epoch)+\", Loss ==> \"+str(last_loss))\n        print(\"Testing Accuracy ==>\")\n        \n        this_valAcc = calAccuracy(model,testloader,device)\n        if(this_valAcc>max_valAcc):\n            max_valAcc = this_valAcc\n            torch.save(model.state_dict(),PATH)\n            print(\"Model saved\")\n        \n    return model\n\n# Running the loop\nPATH = \"/kaggle/working/Transformers.pth\"\ntrainModel(model,trainloader,loss_fn,optimizer,EPOCHS,EPSILON,device,PATH)\n\n\n# reloading the last saved(best) model\nmodel = BertForSequenceClassification.from_pretrained(modelName, num_labels=NUM_LABELS)\nmodel.load_state_dict(torch.load(PATH))\nmodel.to(device)\n\n# print(\"Training Accuracy ==>\"+str(calAccuracy(model,trainloader,device)))\n# print(\"Testing Accuracy ==>\"+str(calAccuracy(model,testloader,device)))\n\n\ncompDataset = bookDataset(\"/kaggle/working/comp_test_x.csv\",\"\",inputPath,tokenizer)\ncomploader = DataLoader(compDataset,batch_size = 64,shuffle = False)\n\n# printing the output dataframe to csv file\ndef outputToFile(model,dataloader,device,outFileName):\n    torch.set_grad_enabled(False)\n    \n    counter = 0\n    Ids = []\n    outputs = []\n    \n    for data in dataloader:\n        input_ids = data[\"input_ids\"].to(device)\n        attention_mask = data[\"attention_mask\"].to(device)\n        model_outputs = model(input_ids,attention_mask = attention_mask)\n        predicted = torch.nn.functional.softmax(model_outputs[\"logits\"]).argmax(1)\n        \n        for pred in predicted:\n            Ids.append(counter)\n            outputs.append(int(pred))\n            counter += 1\n        \n    torch.set_grad_enabled(True)\n    df = pd.DataFrame(list(zip(Ids,outputs)),columns = [\"Id\",\"Genre\"])\n    df.to_csv(outFileName,index=False)\n\noutputToFile(model,comploader,device,\"/kaggle/working/comp_test_y.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-28T17:13:58.922383Z","iopub.execute_input":"2022-11-28T17:13:58.923056Z","iopub.status.idle":"2022-11-28T17:18:45.972881Z","shell.execute_reply.started":"2022-11-28T17:13:58.923016Z","shell.execute_reply":"2022-11-28T17:18:45.971269Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The device in use is : cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd41955432146589471c2b4405534e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a4375f2e131470bb34b8e53e8b98a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482a505ed71847a49bb4f4645d331681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a9c6cce1b2498c9ae07e016382b8d1"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Iteration : 0\nIteration : 100\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_57/4241864303.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m# Running the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/Transformers.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_57/4241864303.py\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, dataloader, loss_fn, optimizer, EPOCHS, EPSILON, device, PATH)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mthis_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}